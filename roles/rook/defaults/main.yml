---
##########################
# operator

operator_user: dragon
operator_group: "{{ operator_user }}"

configuration_directory: /opt/configuration

##########################
# k3s/metallb

metallb_rook_external_IP: 192.168.16.101

##########################
# rook

container_registry_rook: quay.io
rook_ceph_image: "{{ container_registry_rook }}/ceph/ceph"
# renovate: datasource=docker depName=quay.io/rook/ceph
rook_ceph_image_tag: "v18.2"

## template directory containing rook CRD `*.yml.j2` files, e.g. OSISM configuration directory
rook_template_directory: "{{ role_path }}/templates"
rook_configuration_directory: "{{ configuration_directory }}/environments/rook"
## kubernetes namespace in which the rook cluster should be installed
## also see https://rook.io/docs/rook/v1.13/Storage-Configuration/Advanced/ceph-configuration/?h=#using-alternate-namespaces
rook_namespace: rook-ceph

## work directory inside the osism-ansible container
rook_work_directory: /tmp/rook/configuration

## run cleanup tasks and delete cluster
rook_cleanup: false

##########################
# rook CRD
# have a look at `templates/*.yml.j2` to see where exactly the variables are applied
# for ultimate customization, change `{{ rook_template_directory }}` to point to your configuration repository

##############################################
###
### CephCluster
### see https://rook.github.io/docs/rook/latest-release/CRDs/Cluster/ceph-cluster-crd/
### for available configuration
###
##############################################
rook_cluster_name: rook-ceph-cluster
rook_mon_count: 3
rook_mds_count: 3
rook_mgr_count: 3
rook_mgr_modules:
  - name: balancer
    enabled: true
  - name: status
    enabled: true
  - name: prometheus
    enabled: true
  # obtain the information about physical disks in dashboard
  - name: rook
    enabled: true
rook_network_encryption: true
rook_network_compression: true
rook_network_require_msgr2: false
rook_network_public: "192.168.16.0/20"
rook_network_cluster: "{{ rook_network_public }}"
rook_network:
  connections:
    # Whether to encrypt the data in transit across the wire to prevent eavesdropping the data on the network.
    # The default is false. When encryption is enabled, all communication between clients and Ceph daemons, or between Ceph daemons will be encrypted.
    # When encryption is not enabled, clients still establish a strong initial authentication and data integrity is still validated with a crc check.
    # IMPORTANT: Encryption requires the 5.11 kernel for the latest nbd and cephfs drivers. Alternatively for testing only,
    # you can set the "mounter: rbd-nbd" in the rbd storage class, or "mounter: fuse" in the cephfs storage class.
    # The nbd and fuse drivers are *not* recommended in production since restarting the csi driver pod will disconnect the volumes.
    encryption:
      enabled: "{{ rook_network_encryption }}"
    # Whether to compress the data in transit across the wire. The default is false.
    # Requires Ceph Quincy (v17) or newer. Also see the kernel requirements above for encryption.
    compression:
      enabled: "{{ rook_network_compression }}"
    # Whether to require communication over msgr2. If true, the msgr v1 port (6789) will be disabled
    # and clients will be required to connect to the Ceph cluster with the v2 port (3300).
    # Requires a kernel that supports msgr v2 (kernel 5.11 or CentOS 8.4 or newer).
    requireMsgr2: "{{ rook_network_require_msgr2 }}"
  # enable host networking
  provider: host
  addressRanges:
    public:
      - "{{ rook_network_public }}"
    cluster:
      - "{{ rook_network_cluster }}"
rook_crashcollector:
  disable: false
  daysToRetain: 7
rook_logcollector:
  enabled: true
  periodicity: daily  # one of: hourly, daily, weekly, monthly
  maxLogSize: 500M    # SUFFIX may be 'M' or 'G'. Must be at least 1M.
# labels of components have to be equal to the inventory group names regarding each component
rook_placement_label_mds: "rook-mds"
rook_placement_label_mgr: "rook-mgr"
rook_placement_label_mon: "rook-mon"
rook_placement_label_osd: "rook-osd"
rook_placement_label_rgw: "rook-rgw"
rook_placement:
  # also allow to run on kubnernetes master nodes
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
    - key: "node-role.kubernetes.io/master"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
rook_placement_cephcluster:
  all: "{{ rook_placement }}"
  mon:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: "node-role.osism.tech/{{ rook_placement_label_mon }}"
                operator: In
                values:
                  - "true"
  mgr:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: "node-role.osism.tech/{{ rook_placement_label_mgr }}"
                operator: In
                values:
                  - "true"
  osd:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: "node-role.osism.tech/{{ rook_placement_label_osd }}"
                operator: In
                values:
                  - "true"
rook_placement_cephobjectstore:
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
    - key: "node-role.kubernetes.io/master"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: "node-role.osism.tech/{{ rook_placement_label_mds }}"
              operator: In
              values:
                - "true"
rook_placement_cephfilesystem:
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
    - key: "node-role.kubernetes.io/master"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: "node-role.osism.tech/{{ rook_placement_label_rgw }}"
              operator: In
              values:
                - "true"
rook_annotations: {}
rook_annotations_cephcluster: "{{ rook_annotations }}"
rook_annotations_cephfilesystem: "{{ rook_annotations }}"
rook_annotations_cephobjecstore: "{{ rook_annotations }}"
rook_labels: {}
rook_labels_cephcluster: "{{ rook_labels }}"
rook_labels_cephfilesystem: "{{ rook_labels }}"
rook_labels_cephobjecstore: "{{ rook_labels }}"
rook_resources_toolbox:
  limits:
    memory: "1Gi"
  requests:
    cpu: "100m"
    memory: "128Mi"
rook_resources_mgr:
  limits:
    memory: "1Gi"
  requests:
    cpu: "500m"
    memory: "512Mi"
rook_resources_mon:
  limits:
    memory: "2Gi"
  requests:
    cpu: "1000m"
    memory: "1Gi"
rook_resources_osd:
  limits:
    memory: "4Gi"
  requests:
    cpu: "1000m"
    memory: "4Gi"
rook_resources_cephfilesystem:
  limits:
    memory: "4Gi"
  requests:
    cpu: "1000m"
    memory: "4Gi"
rook_resources_cephobjecstore:
  limits:
    memory: "2Gi"
  requests:
    cpu: "1000m"
    memory: "1Gi"
rook_toolbox:
  enabled: true
  image: "{{ rook_ceph_image }}:{{ rook_ceph_image_tag }}"
  tolerations: "{{ rook_placement.tolerations }}"
  affinity: {}
  containerSecurityContext:
    runAsNonRoot: true
    runAsUser: 2016
    runAsGroup: 2016
    capabilities:
      drop: ["ALL"]
  resources: "{{ rook_resources_toolbox }}"
  # priorityClassName:

##############################################
###
### Ceph Dashboard
### see https://rook.github.io/docs/rook/latest-release/Storage-Configuration/Monitoring/ceph-dashboard/
### for available configuration
###
##############################################
rook_dashboard:
  enabled: true
  # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
  # urlPrefix: /ceph-dashboard
  # serve the dashboard at the given port.
  port: 7000
  # Serve the dashboard using SSL (if using ingress to expose the dashboard and `ssl: true` you need to set
  # the corresponding "backend protocol" annotation(s) for your ingress controller of choice)
  ssl: true

##############################################
###
### Monitoring
### see https://rook.github.io/docs/rook/latest-release/Storage-Configuration/Monitoring/ceph-monitoring/
### for available configuration
###
##############################################
rook_monitoring:
  # -- Enable Prometheus integration, will also create necessary RBAC rules to allow Operator to create ServiceMonitors.
  # Monitoring requires Prometheus to be pre-installed
  enabled: false  # TODO: enable
  # -- Whether to create the Prometheus rules for Ceph alerts
  createPrometheusRules: false
  # -- The namespace in which to create the prometheus rules, if different from the rook cluster namespace.
  # If you have multiple rook-ceph clusters in the same k8s cluster, choose the same namespace (ideally, namespace with prometheus
  # deployed) to set rulesNamespaceOverride for all the clusters. Otherwise, you will get duplicate alerts with multiple alert definitions.
  # rulesNamespaceOverride:
  # Monitoring settings for external clusters:
  # externalMgrEndpoints: <list of endpoints>
  # externalMgrPrometheusPort: <port>
  # Scrape interval for prometheus
  # interval: 10s
  # allow adding custom labels and annotations to the prometheus rule
  # prometheusRule:
  #   # -- Labels applied to PrometheusRule
  #   labels: {}
  #   # -- Annotations applied to PrometheusRule
  #   annotations: {}

##############################################
###
### Ceph Config (experimental)
### see https://rook.github.io/docs/rook/latest-release/CRDs/Cluster/ceph-cluster-crd/#ceph-config
### for available configuration
###
##############################################
rook_cephconfig: {}
  # global:
  #   # All values must be quoted so they are considered a string in YAML
  #   osd_pool_default_size: "3"
  #   mon_warn_on_pool_no_redundancy: "false"
  #   osd_crush_update_on_start: "false"
  # # Make sure to quote special characters
  # "osd.*":
  #   osd_max_scrubs: "10"

##############################################
###
### Storage Configuration
### see https://rook.github.io/docs/rook/latest-release/CRDs/Cluster/ceph-cluster-crd/#storage-selection-settings
### for available configuration
###
##############################################
# do not use all nodes
rook_storage_useallnodes: false
# do not use all found devices
rook_storage_usealldevices: false
rook_storage_config_osdsperdevice: "1"
# enable device encryption
rook_storage_config_encrypteddevice: "true"
# define a device filter where to create OSDs
rook_storage_devicefilter: ""
# name nodes where to create OSDs
rook_storage_nodes: []
#  - name: "testbed-node-0"
#  - name: "testbed-node-1"
#  - name: "testbed-node-2"
rook_storage:
  useAllNodes: "{{ rook_storage_useallnodes }}"
  useAllDevices: "{{ rook_storage_usealldevices }}"
  # deviceFilter:
  config:
  #  crushRoot: "custom-root" # specify a non-default root label for the CRUSH map
  #  metadataDevice: "md0" # specify a non-rotational storage so ceph-volume will use it as block db device of bluestore.
  #  databaseSizeMB: "1024" # uncomment if the disks are smaller than 100 GB
    osdsPerDevice: "{{ rook_storage_config_osdsperdevice }}"  # this value can be overridden at the node or device level
    encryptedDevice: "{{ rook_storage_config_encrypteddevice }}"  # the default value for this option is "false"
  # # Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
  # # nodes below will be used as storage resources. Each node's 'name' field should match their 'kubernetes.io/hostname' label.
  # nodes:
  #   - name: "172.17.4.201"
  #     devices: # specific devices to use for storage can be specified for each node
  #       - name: "sdb"
  #       - name: "nvme01" # multiple osds can be created on high performance devices
  #         config:
  #           osdsPerDevice: "5"
  #       - name: "/dev/disk/by-id/ata-ST4000DM004-XXXX" # devices can be specified using full udev paths
  #     config: # configuration can be specified at the node level which overrides the cluster level config
  #   - name: "172.17.4.301"
  #     deviceFilter: "^sd."
  deviceFilter: "{{ rook_storage_devicefilter }}"
  nodes: "{{ rook_storage_nodes }}"

##############################################
###
### CephBlockPool
### see https://rook.io/docs/rook/latest-release/CRDs/Block-Storage/ceph-block-pool-crd/
### for available configuration
###
##############################################
rook_cephblockpool_replicated_default_size: 3
rook_cephblockpool_erasurecoded_default_datachunks: 2
rook_cephblockpool_erasurecoded_default_codingchunks: 1
rook_cephblockpool_default_min_size: "0"
rook_cephblockpool_default_pg_num: "128"
rook_cephblockpools:
  - name: backups
    spec:
      failureDomain: host
      replicated:
        size: "{{ rook_cephblockpool_replicated_default_size }}"
      # erasureCoded:
      #   dataChunks: "{{ rook_cephblockpool_erasurecoded_default_datachunks }}"
      #   codingChunks: "{{ rook_cephblockpool_erasurecoded_default_codingchunks }}"
      parameters:
        min_size: "{{ rook_cephblockpool_default_min_size }}"
        pg_num: "{{ rook_cephblockpool_default_pg_num }}"
        pgp_num: "{{ rook_cephblockpool_default_pg_num }}"
      # deviceClass: ssd
    storageClass:
      enabled: false
  - name: volumes
    spec:
      failureDomain: host
      replicated:
        size: "{{ rook_cephblockpool_replicated_default_size }}"
      # erasureCoded:
      #   dataChunks: "{{ rook_cephblockpool_erasurecoded_default_datachunks }}"
      #   codingChunks: "{{ rook_cephblockpool_erasurecoded_default_codingchunks }}"
      parameters:
        min_size: "{{ rook_cephblockpool_default_min_size }}"
        pg_num: "{{ rook_cephblockpool_default_pg_num }}"
        pgp_num: "{{ rook_cephblockpool_default_pg_num }}"
      # deviceClass: ssd
    storageClass:
      enabled: false
  - name: images
    spec:
      failureDomain: host
      replicated:
        size: "{{ rook_cephblockpool_replicated_default_size }}"
      # erasureCoded:
      #   dataChunks: "{{ rook_cephblockpool_erasurecoded_default_datachunks }}"
      #   codingChunks: "{{ rook_cephblockpool_erasurecoded_default_codingchunks }}"
      parameters:
        min_size: "{{ rook_cephblockpool_default_min_size }}"
        pg_num: "{{ rook_cephblockpool_default_pg_num }}"
        pgp_num: "{{ rook_cephblockpool_default_pg_num }}"
      # deviceClass: ssd
    storageClass:
      enabled: false
  - name: metrics
    spec:
      failureDomain: host
      replicated:
        size: "{{ rook_cephblockpool_replicated_default_size }}"
      # erasureCoded:
      #   dataChunks: "{{ rook_cephblockpool_erasurecoded_default_datachunks }}"
      #   codingChunks: "{{ rook_cephblockpool_erasurecoded_default_codingchunks }}"
      parameters:
        min_size: "{{ rook_cephblockpool_default_min_size }}"
        pg_num: "{{ rook_cephblockpool_default_pg_num }}"
        pgp_num: "{{ rook_cephblockpool_default_pg_num }}"
      # deviceClass: ssd
    storageClass:
      enabled: false
  - name: vms
    spec:
      failureDomain: host
      replicated:
        size: "{{ rook_cephblockpool_replicated_default_size }}"
      # erasureCoded:
      #   dataChunks: "{{ rook_cephblockpool_erasurecoded_default_datachunks }}"
      #   codingChunks: "{{ rook_cephblockpool_erasurecoded_default_codingchunks }}"
      parameters:
        min_size: "{{ rook_cephblockpool_default_min_size }}"
        pg_num: "{{ rook_cephblockpool_default_pg_num }}"
        pgp_num: "{{ rook_cephblockpool_default_pg_num }}"
      # deviceClass: ssd
    storageClass:
      enabled: false

##############################################
###
### CephFilesystem
### see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md
### for available configuration
###
##############################################
rook_cephfilesystem_default_name: cephfs
rook_cephfilesystem_replicated_default_size: 3
rook_cephfilesystem_erasurecoded_datachunks: 2
rook_cephfilesystem_erasurecoded_codingchunks: 1
rook_cephfilesystem_default_metadatapool_parameters_compression_mode: none
rook_cephfilesystem_default_datapool_parameters_compression_mode: none
rook_cephfilesystems:
  - name: "{{ rook_cephfilesystem_default_name }}"
    spec:
      metadataPool:
        failureDomain: host
        # The metadata pool spec must use replication.
        replicated:
          size: "{{ rook_cephfilesystem_replicated_default_size }}"
          requireSafeReplicaSize: true
        parameters:
          compression_mode: "{{ rook_cephfilesystem_default_datapool_parameters_compression_mode }}"
          # target_size_ratio: ".5"
      dataPools:
        - failureDomain: host
          # The data pool spec can use replication or erasure coding.
          replicated:
            size: "{{ rook_cephfilesystem_replicated_default_size }}"
            requireSafeReplicaSize: true
          # erasureCoded:
          #   dataChunks: "{{ rook_cephfilesystem_erasurecoded_default_datachunks }}"
          #   codingChunks: "{{ rook_cephfilesystem_erasurecoded_default_codingchunks }}"
          name: data0
          parameters:
            compression_mode: "{{ rook_cephfilesystem_default_datapool_parameters_compression_mode }}"
            # target_size_ratio: ".5"
      metadataServer:
        activeCount: "{{ rook_mds_count }}"
        activeStandby: true
        resources: "{{ rook_resources_cephfilesystem }}"
        priorityClassName: system-cluster-critical
        placement: "{{ rook_placement_cephfilesystem }}"
        annotations: "{{ rook_annotations_cephfilesystem }}"
    storageClass:
      enabled: false

##############################################
###
### CephObjectStore
### see https://github.com/rook/rook/blob/master/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md
### for available configuration
###
##############################################
rook_cephobjectstore_default_name: rgw
rook_cephobjectstore_replicated_default_size: 3
rook_cephobjectstore_erasurecoded_default_datachunks: 2
rook_cephobjectstore_erasurecoded_default_codingchunks: 1
rook_cephobjectstore_failuredomain: host
rook_cephobjectstore_default_port: 8081
rook_cephobjectstore_preservepoolsondelete: true
rook_cephobjectstores:
  - name: "{{ rook_cephobjectstore_default_name }}"
    spec:
      metadataPool:
        failureDomain: "{{ rook_cephobjectstore_failuredomain }}"
        replicated:
          size: "{{ rook_cephobjectstore_replicated_default_size }}"
        # erasureCoded:
        #   dataChunks: "{{ rook_cephobjectstore_erasurecoded_default_datachunks }}"
        #   codingChunks: "{{ rook_cephobjectstore_erasurecoded_default_codingchunks }}"
      dataPool:
        failureDomain: "{{ rook_cephobjectstore_failuredomain }}"
        replicated:
          size: "{{ rook_cephobjectstore_replicated_default_size }}"
        # erasureCoded:
        #   dataChunks: "{{ rook_cephobjectstore_erasurecoded_default_datachunks }}"
        #   codingChunks: "{{ rook_cephobjectstore_erasurecoded_default_codingchunks }}"
      preservePoolsOnDelete: "{{ rook_cephobjectstore_preservepoolsondelete }}"
      gateway:
        port: "{{ rook_cephobjectstore_default_port }}"
        resources: "{{ rook_resources_cephobjecstore }}"
        # securePort: 443
        # sslCertificateRef:
        instances: 1
        priorityClassName: system-cluster-critical
        placement: "{{ rook_placement_cephobjectstore }}"
        annotations: "{{ rook_annotations_cephobjecstore }}"
    storageClass:
      enabled: false

##############################################
###
### CephClient
### see https://rook.io/docs/rook/latest/CRDs/ceph-client-crd/
### for available configuration
###
##############################################
rook_cephclients:
  cinder-backup:
    caps:
      mon: "profile rbd"
      osd: "profile rbd pool=backups"
    dests:
      - "{{ configuration_directory }}/environments/kolla/files/overlays/cinder/cinder-backup/ceph.client.cinder-backup.keyring"
  cinder:
    caps:
      mon: "profile rbd"
      osd: "profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images"
    dests:
      - "{{ configuration_directory }}/environments/kolla/files/overlays/nova/ceph.client.cinder.keyring"
      - "{{ configuration_directory }}/environments/kolla/files/overlays/cinder/cinder-volume/ceph.client.cinder.keyring"
      - "{{ configuration_directory }}/environments/kolla/files/overlays/cinder/cinder-backup/ceph.client.cinder.keyring"
  glance:
    caps:
      mon: "profile rbd"
      osd: "profile rbd pool=vms, profile rbd pool=images"
    dests:
      - "{{ configuration_directory }}/environments/kolla/files/overlays/glance/ceph.client.glance.keyring"
  gnocchi:
    caps:
      mon: "profile rbd"
      osd: "profile rbd pool=metrics"
    dests:
      - "{{ configuration_directory }}/environments/kolla/files/overlays/gnocchi/ceph.client.gnocchi.keyring"
  nova:
    caps:
      mon: "profile rbd"
      osd: "profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups"
    dests:
      - "{{ configuration_directory }}/environments/kolla/files/overlays/nova/ceph.client.nova.keyring"
  manila:
    caps:
      mon: "allow r"
      mgr: "allow rw"
      osd: "allow rw pool=cephfs_data"
    dests:
      - "{{ configuration_directory }}/environments/kolla/files/overlays/manila/ceph.client.manila.keyring"
