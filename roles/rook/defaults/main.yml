---
##########################
# operator

operator_user: dragon
operator_group: "{{ operator_user }}"

configuration_directory: /opt/configuration

##########################
# rook

container_registry_rook: quay.io
rook_ceph_image: "{{ container_registry_rook }}/ceph/ceph"
# renovate: datasource=docker depName=quay.io/ceph/ceph
rook_ceph_image_tag: 'v19.2.1'

##########################
# rook loadbalancer

rook_loadbalancer_dashboard_port: 80
rook_loadbalancer_dashboard_target_port: "{{ rook_dashboard['port'] }}"

rook_loadbalancer_rgw_port: 80
rook_loadbalancer_rgw_target_port: "{{ rook_cephobjectstore_default_port }}"

##########################
# CephCluster
#
# https://rook.github.io/docs/rook/latest-release/CRDs/Cluster/ceph-cluster-crd/

rook_cluster_name: rook-ceph-cluster
rook_mon_count: 3
rook_mds_count: 3
rook_mgr_count: 3
rook_mgr_modules:
  - name: balancer
    enabled: true
  - name: status
    enabled: true
  - name: prometheus
    enabled: true
  # obtain the information about physical disks in dashboard
  - name: rook
    enabled: true
rook_network_encryption: true
rook_network_compression: true
rook_network_require_msgr2: false
rook_network_public: "192.168.16.0/20"
rook_network_cluster: "{{ rook_network_public }}"
rook_network:
  connections:
    # Whether to encrypt the data in transit across the wire to prevent eavesdropping the data on the network.
    # The default is false. When encryption is enabled, all communication between clients and Ceph daemons, or between Ceph daemons will be encrypted.
    # When encryption is not enabled, clients still establish a strong initial authentication and data integrity is still validated with a crc check.
    # IMPORTANT: Encryption requires the 5.11 kernel for the latest nbd and cephfs drivers. Alternatively for testing only,
    # you can set the "mounter: rbd-nbd" in the rbd storage class, or "mounter: fuse" in the cephfs storage class.
    # The nbd and fuse drivers are *not* recommended in production since restarting the csi driver pod will disconnect the volumes.
    encryption:
      enabled: "{{ rook_network_encryption }}"
    # Whether to compress the data in transit across the wire. The default is false.
    # Requires Ceph Quincy (v17) or newer. Also see the kernel requirements above for encryption.
    compression:
      enabled: "{{ rook_network_compression }}"
    # Whether to require communication over msgr2. If true, the msgr v1 port (6789) will be disabled
    # and clients will be required to connect to the Ceph cluster with the v2 port (3300).
    # Requires a kernel that supports msgr v2 (kernel 5.11 or CentOS 8.4 or newer).
    requireMsgr2: "{{ rook_network_require_msgr2 }}"
  # enable host networking
  provider: host
  addressRanges:
    public:
      - "{{ rook_network_public }}"
    cluster:
      - "{{ rook_network_cluster }}"
rook_crashcollector:
  disable: false
  daysToRetain: 7
rook_logcollector:
  enabled: true
  periodicity: daily  # one of: hourly, daily, weekly, monthly
  maxLogSize: 500M    # SUFFIX may be 'M' or 'G'. Must be at least 1M.
# labels of components have to be equal to the inventory group names regarding each component
rook_placement_label_mds: "rook-mds"
rook_placement_label_mgr: "rook-mgr"
rook_placement_label_mon: "rook-mon"
rook_placement_label_osd: "rook-osd"
rook_placement_label_rgw: "rook-rgw"
rook_placement:
  # also allow to run on kubnernetes master nodes
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
    - key: "node-role.kubernetes.io/master"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
rook_placement_cephcluster:
  all: "{{ rook_placement }}"
  mon:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: "node-role.osism.tech/{{ rook_placement_label_mon }}"
                operator: In
                values:
                  - "true"
  mgr:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: "node-role.osism.tech/{{ rook_placement_label_mgr }}"
                operator: In
                values:
                  - "true"
  osd:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: "node-role.osism.tech/{{ rook_placement_label_osd }}"
                operator: In
                values:
                  - "true"
rook_placement_cephobjectstore:
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
    - key: "node-role.kubernetes.io/master"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: "node-role.osism.tech/{{ rook_placement_label_rgw }}"
              operator: In
              values:
                - "true"
rook_placement_cephfilesystem:
  tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
    - key: "node-role.kubernetes.io/master"
      operator: "Equal"
      value: "true"
      effect: "NoSchedule"
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: "node-role.osism.tech/{{ rook_placement_label_mds }}"
              operator: In
              values:
                - "true"
rook_annotations: {}
rook_annotations_cephcluster: "{{ rook_annotations }}"
rook_annotations_cephfilesystem: "{{ rook_annotations }}"
rook_annotations_cephobjecstore: "{{ rook_annotations }}"
rook_labels: {}
rook_labels_cephcluster: "{{ rook_labels }}"
rook_labels_cephfilesystem: "{{ rook_labels }}"
rook_labels_cephobjecstore: "{{ rook_labels }}"
rook_resources_toolbox:
  limits:
    memory: "1Gi"
  requests:
    cpu: "100m"
    memory: "128Mi"
rook_resources_mgr:
  limits:
    memory: "1Gi"
  requests:
    cpu: "500m"
    memory: "512Mi"
rook_resources_mon:
  limits:
    memory: "2Gi"
  requests:
    cpu: "1000m"
    memory: "1Gi"
rook_resources_osd:
  limits:
    memory: "4Gi"
  requests:
    cpu: "1000m"
    memory: "4Gi"
rook_resources_cephfilesystem:
  limits:
    memory: "4Gi"
  requests:
    cpu: "1000m"
    memory: "4Gi"
rook_resources_cephobjecstore:
  limits:
    memory: "2Gi"
  requests:
    cpu: "1000m"
    memory: "1Gi"
rook_toolbox:
  enabled: true
  image: "{{ rook_ceph_image }}:{{ rook_ceph_image_tag }}"
  tolerations: "{{ rook_placement.tolerations }}"
  affinity: {}
  containerSecurityContext:
    runAsNonRoot: true
    runAsUser: 2016
    runAsGroup: 2016
    capabilities:
      drop: ["ALL"]
  resources: "{{ rook_resources_toolbox }}"
  # priorityClassName:

##########################
# CephDashboard
#
# https://rook.github.io/docs/rook/latest-release/Storage-Configuration/Monitoring/ceph-dashboard/

rook_dashboard:
  enabled: true
  # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
  # urlPrefix: /ceph-dashboard
  # serve the dashboard at the given port.
  port: 7000
  # Serve the dashboard using SSL (if using ingress to expose the dashboard and `ssl: true` you need to set
  # the corresponding "backend protocol" annotation(s) for your ingress controller of choice)
  ssl: false

##########################
# Monitoring
#
# https://rook.github.io/docs/rook/latest-release/Storage-Configuration/Monitoring/ceph-monitoring/

rook_monitoring:
  # -- Enable Prometheus integration, will also create necessary RBAC rules to allow Operator to create ServiceMonitors.
  # Monitoring requires Prometheus to be pre-installed
  enabled: false  # TODO: enable
  # -- Whether to create the Prometheus rules for Ceph alerts
  createPrometheusRules: false
  # -- The namespace in which to create the prometheus rules, if different from the rook cluster namespace.
  # If you have multiple rook-ceph clusters in the same k8s cluster, choose the same namespace (ideally, namespace with prometheus
  # deployed) to set rulesNamespaceOverride for all the clusters. Otherwise, you will get duplicate alerts with multiple alert definitions.
  # rulesNamespaceOverride:
  # Monitoring settings for external clusters:
  # externalMgrEndpoints: <list of endpoints>
  # externalMgrPrometheusPort: <port>
  # Scrape interval for prometheus
  # interval: 10s
  # allow adding custom labels and annotations to the prometheus rule
  # prometheusRule:
  #   # -- Labels applied to PrometheusRule
  #   labels: {}
  #   # -- Annotations applied to PrometheusRule
  #   annotations: {}

##########################
# CephConfig (experimental)
#
# https://rook.github.io/docs/rook/latest-release/CRDs/Cluster/ceph-cluster-crd/#ceph-config

rook_cephconfig: {}
  # global:
  #   # All values must be quoted so they are considered a string in YAML
  #   osd_pool_default_size: "3"
  #   mon_warn_on_pool_no_redundancy: "false"
  #   osd_crush_update_on_start: "false"
  # # Make sure to quote special characters
  # "osd.*":
  #   osd_max_scrubs: "10"

##########################
# Storage Configuration
#
# https://rook.github.io/docs/rook/latest-release/CRDs/Cluster/ceph-cluster-crd/#storage-selection-settings

# do not use all nodes
rook_storage_useallnodes: false
# do not use all found devices
rook_storage_usealldevices: false
rook_storage_config_osdsperdevice: "1"
# enable device encryption
rook_storage_config_encrypteddevice: "true"
# define a device filter where to create OSDs
rook_storage_devicefilter: ""
# name nodes where to create OSDs
rook_storage_nodes: []
#  - name: "testbed-node-0"
#  - name: "testbed-node-1"
#  - name: "testbed-node-2"
rook_storage:
  useAllNodes: "{{ rook_storage_useallnodes }}"
  useAllDevices: "{{ rook_storage_usealldevices }}"
  # deviceFilter:
  config:
  #  crushRoot: "custom-root" # specify a non-default root label for the CRUSH map
  #  metadataDevice: "md0" # specify a non-rotational storage so ceph-volume will use it as block db device of bluestore.
  #  databaseSizeMB: "1024" # uncomment if the disks are smaller than 100 GB
    osdsPerDevice: "{{ rook_storage_config_osdsperdevice }}"  # this value can be overridden at the node or device level
    encryptedDevice: "{{ rook_storage_config_encrypteddevice }}"  # the default value for this option is "false"
  # # Individual nodes and their config can be specified as well, but 'useAllNodes' above must be set to false. Then, only the named
  # # nodes below will be used as storage resources. Each node's 'name' field should match their 'kubernetes.io/hostname' label.
  # nodes:
  #   - name: "172.17.4.201"
  #     devices: # specific devices to use for storage can be specified for each node
  #       - name: "sdb"
  #       - name: "nvme01" # multiple osds can be created on high performance devices
  #         config:
  #           osdsPerDevice: "5"
  #       - name: "/dev/disk/by-id/ata-ST4000DM004-XXXX" # devices can be specified using full udev paths
  #     config: # configuration can be specified at the node level which overrides the cluster level config
  #   - name: "172.17.4.301"
  #     deviceFilter: "^sd."
  deviceFilter: "{{ rook_storage_devicefilter }}"
  nodes: "{{ rook_storage_nodes }}"

##########################
# CephBlockPool
#
# https://rook.io/docs/rook/latest-release/CRDs/Block-Storage/ceph-block-pool-crd/

rook_cephblockpool_replicated_default_size: 3
rook_cephblockpool_erasurecoded_default_datachunks: 2
rook_cephblockpool_erasurecoded_default_codingchunks: 1
rook_cephblockpool_default_min_size: "0"
rook_cephblockpool_default_pg_num: "128"
rook_cephblockpools:
  - name: backups
    spec:
      failureDomain: host
      replicated:
        size: "{{ rook_cephblockpool_replicated_default_size }}"
      # erasureCoded:
      #   dataChunks: "{{ rook_cephblockpool_erasurecoded_default_datachunks }}"
      #   codingChunks: "{{ rook_cephblockpool_erasurecoded_default_codingchunks }}"
      parameters:
        min_size: "{{ rook_cephblockpool_default_min_size }}"
        pg_num: "{{ rook_cephblockpool_default_pg_num }}"
        pgp_num: "{{ rook_cephblockpool_default_pg_num }}"
      # deviceClass: ssd
    storageClass:
      enabled: false
  - name: volumes
    spec:
      failureDomain: host
      replicated:
        size: "{{ rook_cephblockpool_replicated_default_size }}"
      # erasureCoded:
      #   dataChunks: "{{ rook_cephblockpool_erasurecoded_default_datachunks }}"
      #   codingChunks: "{{ rook_cephblockpool_erasurecoded_default_codingchunks }}"
      parameters:
        min_size: "{{ rook_cephblockpool_default_min_size }}"
        pg_num: "{{ rook_cephblockpool_default_pg_num }}"
        pgp_num: "{{ rook_cephblockpool_default_pg_num }}"
      # deviceClass: ssd
    storageClass:
      enabled: false
  - name: images
    spec:
      failureDomain: host
      replicated:
        size: "{{ rook_cephblockpool_replicated_default_size }}"
      # erasureCoded:
      #   dataChunks: "{{ rook_cephblockpool_erasurecoded_default_datachunks }}"
      #   codingChunks: "{{ rook_cephblockpool_erasurecoded_default_codingchunks }}"
      parameters:
        min_size: "{{ rook_cephblockpool_default_min_size }}"
        pg_num: "{{ rook_cephblockpool_default_pg_num }}"
        pgp_num: "{{ rook_cephblockpool_default_pg_num }}"
      # deviceClass: ssd
    storageClass:
      enabled: false
  - name: metrics
    spec:
      failureDomain: host
      replicated:
        size: "{{ rook_cephblockpool_replicated_default_size }}"
      # erasureCoded:
      #   dataChunks: "{{ rook_cephblockpool_erasurecoded_default_datachunks }}"
      #   codingChunks: "{{ rook_cephblockpool_erasurecoded_default_codingchunks }}"
      parameters:
        min_size: "{{ rook_cephblockpool_default_min_size }}"
        pg_num: "{{ rook_cephblockpool_default_pg_num }}"
        pgp_num: "{{ rook_cephblockpool_default_pg_num }}"
      # deviceClass: ssd
    storageClass:
      enabled: false
  - name: vms
    spec:
      failureDomain: host
      replicated:
        size: "{{ rook_cephblockpool_replicated_default_size }}"
      # erasureCoded:
      #   dataChunks: "{{ rook_cephblockpool_erasurecoded_default_datachunks }}"
      #   codingChunks: "{{ rook_cephblockpool_erasurecoded_default_codingchunks }}"
      parameters:
        min_size: "{{ rook_cephblockpool_default_min_size }}"
        pg_num: "{{ rook_cephblockpool_default_pg_num }}"
        pgp_num: "{{ rook_cephblockpool_default_pg_num }}"
      # deviceClass: ssd
    storageClass:
      enabled: false

##########################
# CephFilesystem
#
# https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md

rook_cephfilesystem_default_name: cephfs
rook_cephfilesystem_replicated_default_size: 3
rook_cephfilesystem_erasurecoded_datachunks: 2
rook_cephfilesystem_erasurecoded_codingchunks: 1
rook_cephfilesystem_default_metadatapool_parameters_compression_mode: none
rook_cephfilesystem_default_datapool_parameters_compression_mode: none
rook_cephfilesystems:
  - name: "{{ rook_cephfilesystem_default_name }}"
    spec:
      metadataPool:
        failureDomain: host
        # The metadata pool spec must use replication.
        replicated:
          size: "{{ rook_cephfilesystem_replicated_default_size }}"
          requireSafeReplicaSize: true
        parameters:
          compression_mode: "{{ rook_cephfilesystem_default_datapool_parameters_compression_mode }}"
          # target_size_ratio: ".5"
      dataPools:
        - failureDomain: host
          # The data pool spec can use replication or erasure coding.
          replicated:
            size: "{{ rook_cephfilesystem_replicated_default_size }}"
            requireSafeReplicaSize: true
          # erasureCoded:
          #   dataChunks: "{{ rook_cephfilesystem_erasurecoded_default_datachunks }}"
          #   codingChunks: "{{ rook_cephfilesystem_erasurecoded_default_codingchunks }}"
          name: data0
          parameters:
            compression_mode: "{{ rook_cephfilesystem_default_datapool_parameters_compression_mode }}"
            # target_size_ratio: ".5"
      metadataServer:
        activeCount: "{{ rook_mds_count }}"
        activeStandby: true
        resources: "{{ rook_resources_cephfilesystem }}"
        priorityClassName: system-cluster-critical
        placement: "{{ rook_placement_cephfilesystem }}"
        annotations: "{{ rook_annotations_cephfilesystem }}"
    storageClass:
      enabled: false

##########################
# CephObjectStore
#
# https://github.com/rook/rook/blob/master/Documentation/CRDs/Object-Storage/ceph-object-store-crd.md

rook_cephobjectstore_default_name: rgw
rook_cephobjectstore_replicated_default_size: 3
rook_cephobjectstore_erasurecoded_default_datachunks: 2
rook_cephobjectstore_erasurecoded_default_codingchunks: 1
rook_cephobjectstore_failuredomain: host
rook_cephobjectstore_default_port: 8081
rook_cephobjectstore_preservepoolsondelete: true
rook_cephobjectstore_keystone_acceptedRoles: []
  # - admin
  # - member
rook_cephobjectstore_keystone_implicitTenants: ""
rook_cephobjectstore_keystone_revocationInterval: 1200
rook_cephobjectstore_keystone_tokenCacheSize: 1000
rook_cephobjectstore_keystone_url: ""
rook_cephobjectstore_swift_accountInUrl: true
rook_cephobjectstore_swift_urlPrefix: ""
rook_cephobjectstore_swift_versioningEnabled: true
rook_cephobjectstore_s3_authKeystone: true
rook_cephobjectstore_s3_enable: true
# name of the secret that provides admin user credentials needs to be in same namespace
rook_cephobjectstore_keystone_serviceUserSecretName: ceph-rgw-usersecret
# the following settings belong to the usersecret
rook_cephobjectstore_keystone_auth_type: ""
rook_cephobjectstore_keystone_identity_api_version: 3
rook_cephobjectstore_keystone_password: ""
rook_cephobjectstore_keystone_project_domain_name: "Default"
rook_cephobjectstore_keystone_project_name: ""
rook_cephobjectstore_keystone_user_domain_name: "Default"
rook_cephobjectstore_keystone_username: ""
rook_cephobjectstores:
  - name: "{{ rook_cephobjectstore_default_name }}"
    spec:
      metadataPool:
        failureDomain: "{{ rook_cephobjectstore_failuredomain }}"
        replicated:
          size: "{{ rook_cephobjectstore_replicated_default_size }}"
        # erasureCoded:
        #   dataChunks: "{{ rook_cephobjectstore_erasurecoded_default_datachunks }}"
        #   codingChunks: "{{ rook_cephobjectstore_erasurecoded_default_codingchunks }}"
      dataPool:
        failureDomain: "{{ rook_cephobjectstore_failuredomain }}"
        replicated:
          size: "{{ rook_cephobjectstore_replicated_default_size }}"
        # erasureCoded:
        #   dataChunks: "{{ rook_cephobjectstore_erasurecoded_default_datachunks }}"
        #   codingChunks: "{{ rook_cephobjectstore_erasurecoded_default_codingchunks }}"
      preservePoolsOnDelete: "{{ rook_cephobjectstore_preservepoolsondelete }}"
      gateway:
        port: "{{ rook_cephobjectstore_default_port }}"
        resources: "{{ rook_resources_cephobjecstore }}"
        # securePort: 443
        # sslCertificateRef:
        instances: 1
        priorityClassName: system-cluster-critical
        placement: "{{ rook_placement_cephobjectstore }}"
        annotations: "{{ rook_annotations_cephobjecstore }}"
      auth:
        keystone:
          acceptedRoles: "{{ rook_cephobjectstore_keystone_acceptedRoles }}"
          implicitTenants: "{{ rook_cephobjectstore_keystone_implicitTenants }}"
          revocationInterval: "{{ rook_cephobjectstore_keystone_revocationInterval }}"
          serviceUserSecretName: "{{ rook_cephobjectstore_keystone_serviceUserSecretName }}"
          tokenCacheSize: "{{ rook_cephobjectstore_keystone_tokenCacheSize }}"
          url: "{{ rook_cephobjectstore_keystone_url }}"
        protocols:
          swift:
            accountInUrl: "{{ rook_cephobjectstore_swift_accountInUrl }}"
            urlPrefix: "{{ rook_cephobjectstore_swift_urlPrefix }}"
            versioningEnabled: "{{ rook_cephobjectstore_swift_versioningEnabled }}"
          s3:
            authKeystone: "{{ rook_cephobjectstore_s3_authKeystone }}"
            enable: "{{ rook_cephobjectstore_s3_enable }}"
    storageClass:
      enabled: false

##########################
# CephClient definitions
#
# https://rook.io/docs/rook/latest/CRDs/ceph-client-crd/

rook_ceph_cluster_clients:
  cinder-backup:
    caps:
      mon: "profile rbd"
      osd: "profile rbd pool=backups"
    dests:
      - "{{ configuration_directory }}/environments/kolla/files/overlays/cinder/cinder-backup/ceph.client.cinder-backup.keyring"
  cinder:
    caps:
      mon: "profile rbd"
      osd: "profile rbd pool=volumes, profile rbd pool=vms, profile rbd pool=images"
    dests:
      - "{{ configuration_directory }}/environments/kolla/files/overlays/nova/ceph.client.cinder.keyring"
      - "{{ configuration_directory }}/environments/kolla/files/overlays/cinder/cinder-volume/ceph.client.cinder.keyring"
      - "{{ configuration_directory }}/environments/kolla/files/overlays/cinder/cinder-backup/ceph.client.cinder.keyring"
  glance:
    caps:
      mon: "profile rbd"
      osd: "profile rbd pool=vms, profile rbd pool=images"
    dests:
      - "{{ configuration_directory }}/environments/kolla/files/overlays/glance/ceph.client.glance.keyring"
  gnocchi:
    caps:
      mon: "profile rbd"
      osd: "profile rbd pool=metrics"
    dests:
      - "{{ configuration_directory }}/environments/kolla/files/overlays/gnocchi/ceph.client.gnocchi.keyring"
  nova:
    caps:
      mon: "profile rbd"
      osd: "profile rbd pool=images, profile rbd pool=vms, profile rbd pool=volumes, profile rbd pool=backups"
    dests:
      - "{{ configuration_directory }}/environments/kolla/files/overlays/nova/ceph.client.nova.keyring"
  manila:
    caps:
      mon: "allow r"
      mgr: "allow rw"
      osd: "allow rw pool=cephfs_data"
    dests:
      - "{{ configuration_directory }}/environments/kolla/files/overlays/manila/ceph.client.manila.keyring"

rook_ceph_cluster_helm_release_name: "{{ rook_cluster_name }}"
rook_ceph_cluster_helm_release_namespace: rook-ceph
rook_ceph_cluster_helm_chart_ref: rook-release/rook-ceph-cluster
rook_ceph_cluster_helm_values: []
